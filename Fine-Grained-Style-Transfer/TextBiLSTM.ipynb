{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T07:18:18.450419Z",
     "start_time": "2019-04-25T07:18:16.331252Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "sess_conf = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T07:18:47.914749Z",
     "start_time": "2019-04-25T07:18:47.867475Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear(input_, output_size, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
    "    Args:\n",
    "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term\n",
    "\n",
    "\n",
    "\n",
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class BiLSTM:\n",
    "    def __init__(self, dp, rnn_size, n_layers, num_classes, encoder_embedding_dim, \n",
    "                 sess, lr=0.001, grad_clip=5.0, force_teaching_ratio=1.0, l2_reg_lambda=0,\n",
    "                residual=False, output_keep_prob=0.5, input_keep_prob=0.9, cell_type='lstm', reverse=False,\n",
    "                decay_scheme='luong234'):\n",
    "        \n",
    "        self.rnn_size = rnn_size\n",
    "        self.n_layers = n_layers\n",
    "        self.grad_clip = grad_clip\n",
    "        self.dp = dp\n",
    "        self.num_classes = num_classes\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.residual = residual\n",
    "        self.decay_scheme = decay_scheme\n",
    "        self.l2_reg_lambda = l2_reg_lambda\n",
    "        if self.residual:\n",
    "            assert encoder_embedding_dim == rnn_size\n",
    "        self.reverse = reverse\n",
    "        self.cell_type = cell_type\n",
    "        self.force_teaching_ratio = force_teaching_ratio\n",
    "        self._output_keep_prob = output_keep_prob\n",
    "        self._input_keep_prob = input_keep_prob\n",
    "        self.sess = sess\n",
    "        self.lr=lr\n",
    "        self.build_graph()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep = 35)\n",
    "        #self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        \n",
    "    # end constructor\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.register_symbols()\n",
    "        self.add_input_layer()\n",
    "        self.add_encoder_layer()\n",
    "        self.add_classifer()\n",
    "        self.add_backward_path()\n",
    "    # end method\n",
    "    \n",
    "    def _item_or_tuple(self, seq):\n",
    "        \"\"\"Returns `seq` as tuple or the singular element.\n",
    "        Which is returned is determined by how the AttentionMechanism(s) were passed\n",
    "        to the constructor.\n",
    "        Args:\n",
    "          seq: A non-empty sequence of items or generator.\n",
    "        Returns:\n",
    "           Either the values in the sequence as a tuple if AttentionMechanism(s)\n",
    "           were passed to the constructor as a sequence or the singular element.\n",
    "        \"\"\"\n",
    "        t = tuple(seq)\n",
    "        if self._is_multi:\n",
    "            return t\n",
    "        else:\n",
    "            return t[0]\n",
    "        \n",
    "    def add_input_layer(self):\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, None], name=\"X\")\n",
    "        self.X_seq_len = tf.placeholder(tf.int32, [None], name=\"X_seq_len\")\n",
    "        self.input_y = tf.placeholder(tf.int32, [None, self.num_classes], name='C')\n",
    "        self.input_keep_prob = tf.placeholder(tf.float32,name=\"input_keep_prob\")\n",
    "        self.output_keep_prob = tf.placeholder(tf.float32,name=\"output_keep_prob\")\n",
    "        self.batch_size = tf.shape(self.input_x)[0]\n",
    "        self.l2_loss = tf.constant(0.0)\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    # end method\n",
    "\n",
    "    def single_cell(self, reuse=False):\n",
    "        if self.cell_type == 'lstm':\n",
    "             cell = tf.contrib.rnn.LayerNormBasicLSTMCell(self.rnn_size, reuse=reuse)\n",
    "        else:\n",
    "            cell = tf.contrib.rnn.GRUBlockCell(self.rnn_size)    \n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, self.output_keep_prob, self.input_keep_prob)\n",
    "        if self.residual:\n",
    "            cell = myResidualCell.ResidualWrapper(cell)\n",
    "        return cell\n",
    "    \n",
    "    def add_encoder_layer(self):\n",
    "        encoder_embedding = tf.get_variable('encoder_embedding', [len(self.dp.X_w2id), self.encoder_embedding_dim],\n",
    "                                             tf.float32, tf.random_uniform_initializer(-1.0, 1.0))\n",
    "        \n",
    "        self.encoder_inputs = tf.nn.embedding_lookup(encoder_embedding, self.input_x)\n",
    "        bi_encoder_output, bi_encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw = tf.contrib.rnn.MultiRNNCell([self.single_cell() for _ in range(self.n_layers)]), \n",
    "            cell_bw = tf.contrib.rnn.MultiRNNCell([self.single_cell() for _ in range(self.n_layers)]),\n",
    "            inputs = self.encoder_inputs,\n",
    "            sequence_length = self.X_seq_len,\n",
    "            dtype = tf.float32,\n",
    "            scope = 'bidirectional_rnn')\n",
    "        self.encoder_out = tf.reduce_mean(tf.concat(bi_encoder_output, 2), 1)\n",
    "        print('encoder_out', self.encoder_out)\n",
    "    \n",
    "    def add_classifer(self):\n",
    "        #print('self.encoder_out', self.encoder_out)\n",
    "        with tf.name_scope(\"highway\"):\n",
    "            self.h_highway = highway(self.encoder_out, self.encoder_out.get_shape()[1], 1, 0)\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_highway, self.output_keep_prob)\n",
    "        \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.Variable(tf.truncated_normal([self.rnn_size * 2 * self.n_layers, self.num_classes], stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\n",
    "            self.l2_loss += tf.nn.l2_loss(W)\n",
    "            self.l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.ypred_for_auc = tf.nn.softmax(self.scores)\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "       \n",
    "    def add_backward_path(self):\n",
    "        #print(self.logits, self.C)\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n",
    "            self.d_loss = tf.reshape(tf.reduce_mean(self.loss), shape=[1])\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        params = tf.trainable_variables()\n",
    "        gradients = tf.gradients(self.d_loss, params)\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.grad_clip)\n",
    "        self.learning_rate = tf.constant(self.lr)\n",
    "        self.learning_rate = self.get_learning_rate_decay(self.decay_scheme)  # decay\n",
    "        self.train_op = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)\n",
    "        \n",
    "    def register_symbols(self):\n",
    "        self._x_go = self.dp.X_w2id['<GO>']\n",
    "        self._x_eos = self.dp.X_w2id['<EOS>']\n",
    "        self._x_pad = self.dp.X_w2id['<PAD>']\n",
    "        self._x_unk = self.dp.X_w2id['<UNK>']\n",
    "        \n",
    "        \n",
    "        \n",
    "    def infer(self, x_str):\n",
    "        X_ind = [self.dp.X_w2id[w] for w in x_str.split()]\n",
    "        X_pad_ind = [X_ind]\n",
    "        #print(X_pad_ind)\n",
    "        predict = self.sess.run(self.predictions, \n",
    "                {self.input_x: X_pad_ind,\n",
    "                 self.X_seq_len:[len(X_ind)],\n",
    "                self.output_keep_prob: 1.0,\n",
    "                self.input_keep_prob:1.0})[0]\n",
    "        return predict\n",
    "    \n",
    "    def restore(self, path):\n",
    "        self.saver.restore(self.sess, path)\n",
    "        print('restore %s success' % path)\n",
    "        \n",
    "    def get_learning_rate_decay(self, decay_scheme='luong234'):\n",
    "        num_train_steps = self.dp.num_steps\n",
    "        if decay_scheme == \"luong10\":\n",
    "            start_decay_step = int(num_train_steps / 2)\n",
    "            remain_steps = num_train_steps - start_decay_step\n",
    "            decay_steps = int(remain_steps / 10)  # decay 10 times\n",
    "            decay_factor = 0.5\n",
    "        else:\n",
    "            start_decay_step = int(num_train_steps * 2 / 3)\n",
    "            remain_steps = num_train_steps - start_decay_step\n",
    "            decay_steps = int(remain_steps / 4)  # decay 4 times\n",
    "            decay_factor = 0.5\n",
    "        return tf.cond(\n",
    "            self.global_step < start_decay_step,\n",
    "            lambda: self.learning_rate,\n",
    "            lambda: tf.train.exponential_decay(\n",
    "                self.learning_rate,\n",
    "                (self.global_step - start_decay_step),\n",
    "                decay_steps, decay_factor, staircase=True),\n",
    "            name=\"learning_rate_decay_cond\")\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T07:18:48.505249Z",
     "start_time": "2019-04-25T07:18:48.482773Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN_DP:\n",
    "    def __init__(self, X_indices, C_labels, w2id, batch_size, n_epoch, split_ratio=0.1, test_data=None):\n",
    "        self.n_epoch = n_epoch\n",
    "        if test_data == None:\n",
    "            num_test = int(len(X_indices) * split_ratio)\n",
    "            r = np.random.permutation(len(X_indices))\n",
    "            X_indices = np.array(X_indices)[r].tolist()\n",
    "            C_labels = np.array(C_labels)[r].tolist()\n",
    "            self.C_train = np.array(C_labels[num_test:])\n",
    "            self.X_train = np.array(X_indices[num_test:])\n",
    "            self.C_test = np.array(C_labels[:num_test])\n",
    "            self.X_test = np.array(X_indices[:num_test])\n",
    "        else:\n",
    "            self.X_train, self.C_train, self.X_test, self.C_test = test_data\n",
    "            self.X_train = np.array(self.X_train)\n",
    "            self.C_train = np.array(self.C_train)\n",
    "            self.X_test = np.array(self.X_test)\n",
    "            self.C_test = np.array(self.C_test)\n",
    "        #self.max_length = max_length\n",
    "        self.num_batch = int(len(self.X_train) / batch_size)\n",
    "        self.num_steps = self.num_batch * self.n_epoch\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.X_w2id = w2id\n",
    "        self.X_id2w = dict(zip(w2id.values(), w2id.keys()))\n",
    "        self._x_pad = w2id['<PAD>']\n",
    "        print('Train_data: %d | Test_data: %d | Batch_size: %d | Num_batch: %d | vocab_size: %d' % (len(self.X_train), len(self.X_test), BATCH_SIZE, self.num_batch, len(self.X_w2id)))\n",
    "        \n",
    "    def next_batch(self, X, C):\n",
    "        r = np.random.permutation(len(X))\n",
    "        X = X[r]\n",
    "        C = C[r]\n",
    "        for i in range(0, len(X) - len(X) % self.batch_size, self.batch_size):\n",
    "            X_batch = X[i : i + self.batch_size]\n",
    "            C_batch = C[i : i + self.batch_size]\n",
    "            padded_X_batch, seq_lens = self.pad_sentence_batch(X_batch, self._x_pad)\n",
    "            yield (np.array(padded_X_batch),\n",
    "                   C_batch,\n",
    "                  seq_lens)\n",
    "    \n",
    "    def sample_test_batch(self):\n",
    "        i = random.randint(0, int(len(self.C_test) / self.batch_size)-2)\n",
    "        C = self.C_test[i*self.batch_size:(i+1)*self.batch_size]\n",
    "        padded_X_batch, seq_lens = self.pad_sentence_batch(self.X_test[i*self.batch_size:(i+1)*self.batch_size], self._x_pad)\n",
    "        return np.array(padded_X_batch), C, seq_lens\n",
    "    \n",
    "        \n",
    "    def pad_sentence_batch(self, sentence_batch, pad_int):\n",
    "        padded_seqs = []\n",
    "        seq_lens = []\n",
    "        sentence_batch = sentence_batch.tolist()\n",
    "        max_sentence_len = np.max([len(s) for s in sentence_batch])\n",
    "        for sentence in sentence_batch:\n",
    "            padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "            seq_lens.append(len(sentence))\n",
    "        return padded_seqs, seq_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T07:18:48.865409Z",
     "start_time": "2019-04-25T07:18:48.833903Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN_Util:\n",
    "    def __init__(self, dp, model, display_freq=3):\n",
    "        self.display_freq = display_freq\n",
    "        self.dp = dp\n",
    "        self.D = model\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        avg_c_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        tic = time.time()\n",
    "        X_test_batch, C_test_batch, test_seq_lens  = self.dp.sample_test_batch()\n",
    "        for local_step, (X_train_batch, C_train_batch, seq_lens) in enumerate(\n",
    "            self.dp.next_batch(self.dp.X_train, self.dp.C_train)):\n",
    "            #print(len(C_train_batch), len(X_train_batch))\n",
    "            acc, loss, _ = self.D.sess.run([self.D.accuracy, self.D.d_loss, self.D.train_op], \n",
    "                {self.D.input_x: X_train_batch, \n",
    "                 self.D.input_y: C_train_batch, \n",
    "                 self.D.X_seq_len:seq_lens,\n",
    "                 self.D.output_keep_prob:self.D._output_keep_prob,\n",
    "                 self.D.input_keep_prob:self.D._input_keep_prob})\n",
    "            avg_c_loss += loss\n",
    "            avg_acc += acc\n",
    "            if (local_step % int(self.dp.num_batch / self.display_freq)) == 0:\n",
    "                val_acc, val_c_loss = self.D.sess.run([self.D.accuracy, self.D.d_loss], \n",
    "                                            {self.D.input_x: X_test_batch, \n",
    "                                            self.D.input_y: C_test_batch, \n",
    "                                            self.D.X_seq_len:test_seq_lens,\n",
    "                                             self.D.output_keep_prob:1.0,\n",
    "                                             self.D.input_keep_prob:1.0})\n",
    "                print(\"Epoch %d/%d | Batch %d/%d | Train_loss: %.3f Acc %.3f | Test_loss: %.3f Acc %.3f | Time_cost:%.3f\" % \n",
    "                      (epoch, self.n_epoch, local_step, self.dp.num_batch, avg_c_loss / (local_step + 1), avg_acc / (local_step + 1), val_c_loss, val_acc, time.time()-tic))\n",
    "                self.cal()\n",
    "                tic = time.time()\n",
    "        return avg_c_loss / (local_step + 1), avg_acc / (local_step + 1)\n",
    "    \n",
    "    def test(self):\n",
    "        avg_c_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        tic = time.time()\n",
    "        for local_step, (X_test_batch, C_test_batch, test_seq_lens) in enumerate(\n",
    "            self.dp.next_batch(self.dp.X_test, self.dp.C_test)):\n",
    "            acc, loss = self.D.sess.run([self.D.accuracy, self.D.d_loss], \n",
    "               {self.D.input_x: X_test_batch, \n",
    "                self.D.input_y: C_test_batch, \n",
    "                self.D.X_seq_len:test_seq_lens,\n",
    "                 self.D.output_keep_prob:1.0,\n",
    "                 self.D.input_keep_prob:1.0})\n",
    "            avg_c_loss += loss\n",
    "            avg_acc += acc\n",
    "        return avg_c_loss / (local_step + 1), avg_acc / (local_step + 1)\n",
    "    \n",
    "    def fit(self, train_dir):\n",
    "        self.n_epoch = self.dp.n_epoch\n",
    "        out_dir = train_dir\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        print(\"Writing to %s\" % out_dir)\n",
    "        checkpoint_prefix = os.path.join(out_dir, \"model\")\n",
    "        self.summary_writer = tf.summary.FileWriter(os.path.join(out_dir, 'Summary'), self.D.sess.graph)\n",
    "        for epoch in range(1, self.n_epoch+1):\n",
    "            tic = time.time()\n",
    "            train_c_loss, train_acc = self.train(epoch)\n",
    "            test_c_loss, test_acc = self.test()\n",
    "            print(\"Epoch %d/%d | Train_loss: %.3f Acc %.3f | Test_loss: %.3f Acc %.3f\" % \n",
    "                  (epoch, self.n_epoch, train_c_loss, train_acc, test_c_loss, test_acc))\n",
    "            path = self.D.saver.save(self.D.sess, checkpoint_prefix, global_step=epoch)\n",
    "            print(\"Saved model checkpoint to %s\" % path)\n",
    "    \n",
    "    def show(self, sent, id2w):\n",
    "        return \" \".join([id2w.get(idx, u'&') for idx in sent])\n",
    "    \n",
    "    def cal(self, n_example=5):\n",
    "        train_n_example = int(n_example / 2)\n",
    "        test_n_example = n_example - train_n_example\n",
    "        for _ in random.sample([t for t in range(len(self.dp.X_test))], test_n_example):\n",
    "            example = self.show(self.dp.X_test[_], self.dp.X_id2w)\n",
    "            o = self.D.infer(example)\n",
    "            print('Test Input: %s | Output: %d | GroundTruth: %d' % (example, o, np.argmax(self.dp.C_test[_])))\n",
    "        for _ in random.sample([t for t in range(len(self.dp.X_train))], train_n_example):\n",
    "            example = self.show(self.dp.X_train[_], self.dp.X_id2w)\n",
    "            o = self.D.infer(example)\n",
    "            print('Train Input: %s | Output: %d | GroundTruth: %d' % (example, o, np.argmax(self.dp.C_train[_]))) \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T07:18:51.882114Z",
     "start_time": "2019-04-25T07:18:49.607589Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "w2id, id2w, X_indices, C_labels = pickle.load(open('/workspace/Data/yelp/w2id_id2w_indices_labels_all.pkl','rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T07:19:16.062717Z",
     "start_time": "2019-04-25T07:19:06.638977Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCH = 30\n",
    "train_dir ='Model/YELP/TextBiLSTM-all/'\n",
    "#MAX_LENGTH = 16\n",
    "\n",
    "\n",
    "dp = TextCNN_DP(X_indices, C_labels, w2id,  BATCH_SIZE, n_epoch=NUM_EPOCH, test_data=None)\n",
    "\n",
    "g2 = tf.Graph()\n",
    "sess2 = tf.Session(graph=g2, config=sess_conf) \n",
    "with sess2.as_default():\n",
    "    with sess2.graph.as_default():\n",
    "        model = BiLSTM(\n",
    "            dp = dp,\n",
    "            rnn_size = 512,\n",
    "            n_layers = 1,\n",
    "            encoder_embedding_dim = 128,\n",
    "            cell_type = 'lstm',\n",
    "            num_classes = 2,\n",
    "            sess=sess2\n",
    "        )\n",
    "        print(len(tf.global_variables()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-25T11:28:40.170869Z",
     "start_time": "2019-04-25T07:19:16.066386Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util = TextCNN_Util(dp=dp, model=model)\n",
    "util.fit(train_dir=train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
