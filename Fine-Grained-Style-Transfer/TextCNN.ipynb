{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T06:04:21.844393Z",
     "start_time": "2019-04-02T06:04:19.953231Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "sess_conf = tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_614564/337460670.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 19:01:03.898869: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T06:04:21.883876Z",
     "start_time": "2019-04-02T06:04:21.846827Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "def linear(input_, output_size, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
    "    Args:\n",
    "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        \n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.compat.v1.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.compat.v1.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.compat.v1.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.compat.v1.matmul(input_, tf.compat.v1.transpose(matrix)) + bias_term\n",
    "\n",
    "\n",
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.compat.v1.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, sess, sequence_length, num_classes, vocab_size, dp,\n",
    "            emd_dim, filter_sizes, num_filters, l2_reg_lambda=0.0, dropout_keep_prob = 1):\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.compat.v1.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.compat.v1.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_rate = dropout_keep_prob\n",
    "        self.dropout_input = tf.compat.v1.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.sess = sess\n",
    "        self.max_sentence_len = sequence_length\n",
    "        self.dp = dp\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.compat.v1.constant(0.0)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('TextCNN'):\n",
    "            # Embedding layer\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "                self.W = tf.Variable(\n",
    "                    tf.compat.v1.random_uniform([vocab_size, emd_dim], -1.0, 1.0),\n",
    "                    name=\"W\")\n",
    "                self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "            # Create a convolution + maxpool layer for each filter size\n",
    "            pooled_outputs = []\n",
    "            for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
    "                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                    # Convolution Layer\n",
    "                    filter_shape = [filter_size, emd_dim, 1, num_filter]\n",
    "                    W = tf.Variable(tf.compat.v1.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[num_filter]), name=\"b\")\n",
    "                    conv = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded,\n",
    "                        W,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\")\n",
    "                    # Apply nonlinearity\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                    # Maxpooling over the outputs\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=\"pool\")\n",
    "                    pooled_outputs.append(pooled)\n",
    "\n",
    "            # Combine all the pooled features\n",
    "            num_filters_total = sum(num_filters)\n",
    "            self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "            # Add highway\n",
    "            with tf.name_scope(\"highway\"):\n",
    "                self.h_highway = highway(self.h_pool_flat, self.h_pool_flat.get_shape()[1], 1, 0)\n",
    "\n",
    "            # Add dropout\n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_input)\n",
    "\n",
    "            # Final (unnormalized) scores and predictions\n",
    "            with tf.name_scope(\"output\"):\n",
    "                W = tf.Variable(tf.compat.v1.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "                self.scores = tf.compat.v1.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "                self.ypred_for_auc = tf.nn.softmax(self.scores)\n",
    "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "            # CalculateMean cross-entropy loss\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "                self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "                self.d_loss = tf.reshape(tf.reduce_mean(self.loss), shape=[1])\n",
    "                \n",
    "            with tf.name_scope(\"accuracy\"):\n",
    "                correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "                \n",
    "        self.params = tf.compat.v1.trainable_variables()\n",
    "        d_optimizer = tf.compat.v1.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = d_optimizer.compute_gradients(self.loss, self.params, aggregation_method=2)\n",
    "        self.train_op = d_optimizer.apply_gradients(grads_and_vars)\n",
    "        #self.saver = tf.train.Saver([v for v in tf.trainable_variables() if 'summary_' not in v.name], max_to_keep = 5)\n",
    "        self.saver = tf.compat.v1.train.Saver(tf.compat.v1.trainable_variables(), max_to_keep = 5)\n",
    "        self.summary_placeholders, self.update_ops, self.summary_op = self.setup_summary()\n",
    "        self.sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        \n",
    "    def save(self, path, epoch):\n",
    "        checkpoint_prefix = os.path.join(path, \"model\")\n",
    "        self.saver.save(self.sess, checkpoint_prefix, global_step=epoch)\n",
    "        print('save to %s success' % checkpoint_prefix)\n",
    "        \n",
    "    def restore(self, path):\n",
    "        self.saver.restore(self.sess, path)\n",
    "        print('restore %s success' % path)\n",
    "        \n",
    "    def setup_summary(self):\n",
    "        train_loss_ = tf.Variable(0., name='summary_train_loss')\n",
    "        tf.compat.v1.summary.scalar('Train_loss', train_loss_)\n",
    "        train_acc_ = tf.Variable(0., name='summary_train_acc')\n",
    "        tf.compat.v1.summary.scalar('Train_Acc', train_acc_)\n",
    "        \n",
    "        test_loss_ = tf.Variable(0., name='summary_train_loss')\n",
    "        tf.compat.v1.summary.scalar('Train_loss', test_loss_)\n",
    "        test_acc_ = tf.Variable(0., name='summary_test_acc')\n",
    "        tf.compat.v1.summary.scalar('Test_Acc', test_acc_)\n",
    "        \n",
    "        summary_vars = [train_loss_, train_acc_, test_loss_, test_acc_]\n",
    "        summary_placeholders = [tf.compat.v1.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "        update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "        summary_op = tf.compat.v1.summary.merge_all()\n",
    "        return summary_placeholders, update_ops, summary_op\n",
    "\n",
    "    def infer(self, x_str):\n",
    "        X_ind = [self.dp.w2id[w] for w in x_str.split()]\n",
    "        X_pad_ind = [X_ind + [self.dp._x_pad] * (self.dp.max_length - len(X_ind))]\n",
    "        #print(X_pad_ind)\n",
    "        predict = self.sess.run(self.predictions, \n",
    "                {self.input_x: X_pad_ind,\n",
    "                self.dropout_input: 1.0})[0]\n",
    "        return predict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T06:11:09.820459Z",
     "start_time": "2019-04-02T06:11:09.800014Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN_DP:\n",
    "    def __init__(self, X_indices, C_labels, w2id, batch_size, max_length, n_epoch, split_ratio=0.1, test_data=None):\n",
    "        self.n_epoch = n_epoch\n",
    "        if test_data == None:\n",
    "            num_test = int(len(X_indices) * split_ratio)\n",
    "            r = np.random.permutation(len(X_indices))\n",
    "            X_indices = np.array(X_indices)[r].tolist()\n",
    "            C_labels = np.array(C_labels)[r].tolist()\n",
    "            self.C_train = np.array(C_labels[num_test:])\n",
    "            self.X_train = np.array(X_indices[num_test:])\n",
    "            self.C_test = np.array(C_labels[:num_test])\n",
    "            self.X_test = np.array(X_indices[:num_test])\n",
    "        else:\n",
    "            self.X_train, self.C_train, self.X_test, self.C_test = test_data\n",
    "            self.X_train = np.array(self.X_train, dtype=object)\n",
    "            self.C_train = np.array(self.C_train, dtype=object)\n",
    "            self.X_test = np.array(self.X_test, dtype=object)\n",
    "            self.C_test = np.array(self.C_test, dtype=object)\n",
    "        self.max_length = max_length\n",
    "        self.num_batch = int(len(self.X_train) / batch_size)\n",
    "        self.num_steps = self.num_batch * self.n_epoch\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.w2id = w2id\n",
    "        self.id2w = dict(zip(w2id.values(), w2id.keys()))\n",
    "        self._x_pad = w2id['<PAD>']\n",
    "        print('Train_data: %d | Test_data: %d | Batch_size: %d | Num_batch: %d | vocab_size: %d' % (len(self.X_train), len(self.X_test), BATCH_SIZE, self.num_batch, len(self.w2id)))\n",
    "        \n",
    "    def next_batch(self, X, C):\n",
    "        r = np.random.permutation(len(X))\n",
    "        X = X[r]\n",
    "        C = C[r]\n",
    "        for i in range(0, len(X) - len(X) % self.batch_size, self.batch_size):\n",
    "            X_batch = X[i : i + self.batch_size]\n",
    "            C_batch = C[i : i + self.batch_size]\n",
    "            padded_X_batch = self.pad_sentence_batch(X_batch, self._x_pad)\n",
    "            yield (np.array(padded_X_batch),\n",
    "                   C_batch)\n",
    "    \n",
    "    def sample_test_batch(self):\n",
    "        i = random.randint(0, int(len(self.C_test) / self.batch_size)-2)\n",
    "        C = self.C_test[i*self.batch_size:(i+1)*self.batch_size]\n",
    "        padded_X_batch = self.pad_sentence_batch(self.X_test[i*self.batch_size:(i+1)*self.batch_size], self._x_pad)\n",
    "        return np.array(padded_X_batch), C\n",
    "    \n",
    "        \n",
    "    def pad_sentence_batch(self, sentence_batch, pad_int):\n",
    "        padded_seqs = []\n",
    "        seq_lens = []\n",
    "        sentence_batch = sentence_batch.tolist()\n",
    "        max_sentence_len = self.max_length\n",
    "        for sentence in sentence_batch:\n",
    "            padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "            seq_lens.append(len(sentence))\n",
    "        return padded_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T06:08:56.493993Z",
     "start_time": "2019-04-02T06:08:56.461144Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextCNN_Util:\n",
    "    def __init__(self, dp, model, display_freq=3):\n",
    "        self.display_freq = display_freq\n",
    "        self.dp = dp\n",
    "        self.D = model\n",
    "        \n",
    "    def train(self, epoch):\n",
    "        avg_c_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        tic = time.time()\n",
    "        X_test_batch, C_test_batch  = self.dp.sample_test_batch()\n",
    "        for local_step, (X_train_batch, C_train_batch) in enumerate(\n",
    "            self.dp.next_batch(self.dp.X_train, self.dp.C_train)):\n",
    "            #print(len(C_train_batch), len(X_train_batch))\n",
    "            acc, loss, _ = self.D.sess.run([self.D.accuracy, self.D.d_loss, self.D.train_op], \n",
    "                {self.D.input_x: X_train_batch, \n",
    "                self.D.input_y: C_train_batch, \n",
    "                self.D.dropout_input: self.D.dropout_rate})\n",
    "            avg_c_loss += loss\n",
    "            avg_acc += acc\n",
    "            if (local_step % int(self.dp.num_batch / self.display_freq)) == 0:\n",
    "                val_acc, val_c_loss = self.D.sess.run([self.D.accuracy, self.D.d_loss], \n",
    "                                            {self.D.input_x: X_test_batch, \n",
    "                                            self.D.input_y: C_test_batch, \n",
    "                                            self.D.dropout_input: self.D.dropout_rate})\n",
    "                print(\"Epoch %d/%d | Batch %d/%d | Train_loss: %.3f Acc %.3f | Test_loss: %.3f Acc %.3f | Time_cost:%.3f\" % \n",
    "                      (epoch, self.n_epoch, local_step, self.dp.num_batch, avg_c_loss / (local_step + 1), avg_acc / (local_step + 1), val_c_loss, val_acc, time.time()-tic))\n",
    "                self.cal()\n",
    "                tic = time.time()\n",
    "        return avg_c_loss / (local_step + 1), avg_acc / (local_step + 1)\n",
    "    \n",
    "    def test(self):\n",
    "        avg_c_loss = 0.0\n",
    "        avg_acc = 0.0\n",
    "        tic = time.time()\n",
    "        for local_step, (X_test_batch, C_test_batch) in enumerate(\n",
    "            self.dp.next_batch(self.dp.X_test, self.dp.C_test)):\n",
    "            acc, loss = self.D.sess.run([self.D.accuracy, self.D.d_loss], \n",
    "                {self.D.input_x: X_test_batch, \n",
    "                self.D.input_y: C_test_batch, \n",
    "                self.D.dropout_input: 1.0})\n",
    "            avg_c_loss += loss\n",
    "            avg_acc += acc\n",
    "        return avg_c_loss / (local_step + 1), avg_acc / (local_step + 1)\n",
    "    \n",
    "    def fit(self, train_dir):\n",
    "        self.n_epoch = self.dp.n_epoch\n",
    "        out_dir = train_dir\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        print(\"Writing to %s\" % out_dir)\n",
    "        checkpoint_prefix = os.path.join(out_dir, \"model\")\n",
    "        # self.summary_writer = tf.compat.v1.summary.FileWriter(os.path.join(out_dir, 'Summary'), self.D.sess.graph)\n",
    "        for epoch in range(1, self.n_epoch+1):\n",
    "            tic = time.time()\n",
    "            train_c_loss, train_acc = self.train(epoch)\n",
    "            test_c_loss, test_acc = self.test()\n",
    "            print(\"Epoch %d/%d | Train_loss: %.3f Acc %.3f | Test_loss: %.3f Acc %.3f\" % \n",
    "                  (epoch, self.n_epoch, train_c_loss, train_acc, test_c_loss, test_acc))\n",
    "            path = self.D.saver.save(self.D.sess, checkpoint_prefix, global_step=epoch)\n",
    "            print(\"Saved model checkpoint to %s\" % path)\n",
    "    \n",
    "    def show(self, sent, id2w):\n",
    "        return \" \".join([id2w.get(idx, u'&') for idx in sent])\n",
    "    \n",
    "    def cal(self, n_example=5):\n",
    "        train_n_example = int(n_example / 2)\n",
    "        test_n_example = n_example - train_n_example\n",
    "        for _ in random.sample([t for t in range(len(self.dp.X_test))], test_n_example):\n",
    "            example = self.show(self.dp.X_test[_], self.dp.id2w)\n",
    "            o = self.D.infer(example)\n",
    "            print('Test Input: %s | Output: %d | GroundTruth: %d' % (example, o, np.argmax(self.dp.C_test[_])))\n",
    "        for _ in random.sample([t for t in range(len(self.dp.X_train))], train_n_example):\n",
    "            example = self.show(self.dp.X_train[_], self.dp.id2w)\n",
    "            o = self.D.infer(example)\n",
    "            print('Train Input: %s | Output: %d | GroundTruth: %d' % (example, o, np.argmax(self.dp.C_train[_]))) \n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T06:08:59.771102Z",
     "start_time": "2019-04-02T06:08:57.637108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "w2id, id2w = pickle.load(open('AEGS_data/yelp/w2id_id2w.pkl','rb'))\n",
    "Y_train, C_train = pickle.load(open('AEGS_data/yelp/XC_train.pkl','rb'))\n",
    "Y_dev, C_dev = pickle.load(open('AEGS_data/yelp/XC_dev.pkl','rb'))\n",
    "Y_test, C_test = pickle.load(open('AEGS_data/yelp/XC_test.pkl','rb'))\n",
    "print(C_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T06:09:00.628403Z",
     "start_time": "2019-04-02T06:08:59.774901Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = [x[:-1] for x in Y_train]\n",
    "X_dev = [x[:-1] for x in Y_dev]\n",
    "X_test = [x[:-1] for x in Y_test]\n",
    "#print(idx2str(Y_test[0]), idx2str(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(X_dev[0])\n",
    "# len(X_dev), len(X_dev[0]), X_dev[0]\n",
    "# for i in X_dev:\n",
    "#     if (len(i) != 9):\n",
    "#         print(len(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T06:11:13.826887Z",
     "start_time": "2019-04-02T06:11:12.278451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data: 447259 | Test_data: 63894 | Batch_size: 256 | Num_batch: 1747 | vocab_size: 9361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:48:33.406973: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-11-08 18:48:33.875287: W tensorflow/c/c_api.cc:291] Operation '{name:'TextCNN/embedding/W/Adam_1/Assign' id:584 op device:{requested: '/device:CPU:0', assigned: ''} def:{{{node TextCNN/embedding/W/Adam_1/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false, _device=\"/device:CPU:0\"](TextCNN/embedding/W/Adam_1, TextCNN/embedding/W/Adam_1/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCH = 30\n",
    "train_dir ='Model/YELP/TextCNN/'\n",
    "MAX_LENGTH = 16\n",
    "\n",
    "dp = TextCNN_DP(None, None, w2id,  BATCH_SIZE, max_length = MAX_LENGTH, n_epoch=NUM_EPOCH, test_data=(X_train, C_train, X_dev, C_dev))\n",
    "\n",
    "emb_dim = 128\n",
    "filter_sizes = [1, 2, 3, 4, 5]\n",
    "num_filters = [128, 128, 128, 128, 128]\n",
    "\n",
    "g2 = tf.Graph()\n",
    "sess2 = tf.compat.v1.Session(graph=g2, config=sess_conf) \n",
    "with sess2.as_default():\n",
    "    with sess2.graph.as_default():\n",
    "        D = TextCNN(sess = sess2, dp = dp, sequence_length=MAX_LENGTH, num_classes=2, vocab_size=len(dp.id2w),\n",
    "                          emd_dim = emb_dim, filter_sizes = filter_sizes, num_filters=num_filters,\n",
    "                          l2_reg_lambda=0.2, dropout_keep_prob=0.75)\n",
    "        D.sess.run(tf.compat.v1.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrainD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-02T06:28:17.441628Z",
     "start_time": "2019-04-02T06:11:13.830141Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/ug21/deepasha/anaconda3/envs/style/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Writing to Model/YELP/TextCNN/\n",
      "Epoch 1/30 | Batch 0/1747 | Train_loss: 0.474 Acc 0.898 | Test_loss: 0.487 Acc 0.891 | Time_cost:0.184\n",
      "Test Input: would not recommend . | Output: 0 | GroundTruth: 0\n",
      "Test Input: love golfing this course ! | Output: 0 | GroundTruth: 1\n",
      "Test Input: the have great frozen coffee . | Output: 0 | GroundTruth: 1\n",
      "Train Input: this place is no way close to yummy . | Output: 0 | GroundTruth: 0\n",
      "Train Input: the staff are friendly and courteous . | Output: 0 | GroundTruth: 1\n",
      "\n",
      "Epoch 1/30 | Batch 582/1747 | Train_loss: 0.377 Acc 0.908 | Test_loss: 0.352 Acc 0.902 | Time_cost:53.265\n",
      "Test Input: great sandwiches and authentic italian food . | Output: 0 | GroundTruth: 1\n",
      "Test Input: i have eaten here about _num_ times now and it has always been fantastic ! | Output: 0 | GroundTruth: 1\n",
      "Test Input: ok , so normal delivery form this place is maybe a 3-4 . | Output: 0 | GroundTruth: 0\n",
      "Train Input: too bad ! | Output: 0 | GroundTruth: 0\n",
      "Train Input: it was pricey but worth it in my eyes . | Output: 0 | GroundTruth: 1\n",
      "\n",
      "Epoch 1/30 | Batch 1164/1747 | Train_loss: 0.320 Acc 0.916 | Test_loss: 0.290 Acc 0.906 | Time_cost:53.093\n",
      "Test Input: it was so nice having a fireplace in the room ! | Output: 0 | GroundTruth: 1\n",
      "Test Input: they tend to short scoop so you feel like you 're getting ripped off . | Output: 0 | GroundTruth: 0\n",
      "Test Input: i barely lived in this apartment . | Output: 0 | GroundTruth: 0\n",
      "Train Input: i called the server back over and sent the dish back . | Output: 0 | GroundTruth: 0\n",
      "Train Input: great place to go for a walk around the lake . | Output: 0 | GroundTruth: 1\n",
      "\n",
      "Epoch 1/30 | Batch 1746/1747 | Train_loss: 0.282 Acc 0.921 | Test_loss: 0.226 Acc 0.914 | Time_cost:53.064\n",
      "Test Input: well , what do you know , i loved it ! | Output: 0 | GroundTruth: 1\n",
      "Test Input: i love my hair ! | Output: 0 | GroundTruth: 1\n",
      "Test Input: was worth only $ _num_ . | Output: 0 | GroundTruth: 0\n",
      "Train Input: the brie brle was so delicious . | Output: 0 | GroundTruth: 1\n",
      "Train Input: wow , dirty bathrooms , dirty concession stand . | Output: 0 | GroundTruth: 0\n",
      "\n",
      "Epoch 1/30 | Train_loss: 0.282 Acc 0.921 | Test_loss: 0.720 Acc 0.401\n",
      "Saved model checkpoint to Model/YELP/TextCNN/model-1\n",
      "Epoch 2/30 | Batch 0/1747 | Train_loss: 0.221 Acc 0.926 | Test_loss: 0.150 Acc 0.949 | Time_cost:0.160\n",
      "Test Input: the beef enchilada my husband had was fantastic and tostada was delicious . | Output: 0 | GroundTruth: 1\n",
      "Test Input: love the pork green chile . | Output: 0 | GroundTruth: 1\n",
      "Test Input: reasonable price tickets and a delightful show . | Output: 0 | GroundTruth: 1\n",
      "Train Input: you will find something for everyone . | Output: 0 | GroundTruth: 1\n",
      "Train Input: these , however , are not normal circumstances . | Output: 0 | GroundTruth: 0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mdisable_v2_behavior()\n\u001b[1;32m      2\u001b[0m util \u001b[39m=\u001b[39m TextCNN_Util(dp\u001b[39m=\u001b[39mdp, model\u001b[39m=\u001b[39mD)\n\u001b[0;32m----> 3\u001b[0m util\u001b[39m.\u001b[39;49mfit(train_dir\u001b[39m=\u001b[39;49mtrain_dir)\n",
      "Cell \u001b[0;32mIn[22], line 56\u001b[0m, in \u001b[0;36mTextCNN_Util.fit\u001b[0;34m(self, train_dir)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     55\u001b[0m     tic \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 56\u001b[0m     train_c_loss, train_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(epoch)\n\u001b[1;32m     57\u001b[0m     test_c_loss, test_acc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest()\n\u001b[1;32m     58\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m | Train_loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m Acc \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m | Test_loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m Acc \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \n\u001b[1;32m     59\u001b[0m           (epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_epoch, train_c_loss, train_acc, test_c_loss, test_acc))\n",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m, in \u001b[0;36mTextCNN_Util.train\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m X_test_batch, C_test_batch  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdp\u001b[39m.\u001b[39msample_test_batch()\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m local_step, (X_train_batch, C_train_batch) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\n\u001b[1;32m     13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdp\u001b[39m.\u001b[39mnext_batch(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdp\u001b[39m.\u001b[39mX_train, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdp\u001b[39m.\u001b[39mC_train)):\n\u001b[1;32m     14\u001b[0m     \u001b[39m#print(len(C_train_batch), len(X_train_batch))\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     acc, loss, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49msess\u001b[39m.\u001b[39;49mrun([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49maccuracy, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49md_loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49mtrain_op], \n\u001b[1;32m     16\u001b[0m         {\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49minput_x: X_train_batch, \n\u001b[1;32m     17\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49minput_y: C_train_batch, \n\u001b[1;32m     18\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49mdropout_input: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mD\u001b[39m.\u001b[39;49mdropout_rate})\n\u001b[1;32m     19\u001b[0m     avg_c_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     20\u001b[0m     avg_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\n",
      "File \u001b[0;32m~/anaconda3/envs/style/lib/python3.8/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39mNone\u001b[39;49;00m, fetches, feed_dict, options_ptr,\n\u001b[1;32m    969\u001b[0m                      run_metadata_ptr)\n\u001b[1;32m    970\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/anaconda3/envs/style/lib/python3.8/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[39m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[39m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[39mif\u001b[39;00m final_fetches \u001b[39mor\u001b[39;00m final_targets \u001b[39mor\u001b[39;00m (handle \u001b[39mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_run(handle, final_targets, final_fetches,\n\u001b[1;32m   1192\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/style/lib/python3.8/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m   1372\u001b[0m                        run_metadata)\n\u001b[1;32m   1373\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/anaconda3/envs/style/lib/python3.8/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_do_call\u001b[39m(\u001b[39mself\u001b[39m, fn, \u001b[39m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m   1379\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOpError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_text(e\u001b[39m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/anaconda3/envs/style/lib/python3.8/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[39m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m   1362\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[0;32m~/anaconda3/envs/style/lib/python3.8/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_tf_sessionrun\u001b[39m(\u001b[39mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[39mreturn\u001b[39;00m tf_session\u001b[39m.\u001b[39;49mTF_SessionRun_wrapper(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session, options, feed_dict,\n\u001b[1;32m   1455\u001b[0m                                           fetch_list, target_list,\n\u001b[1;32m   1456\u001b[0m                                           run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_v2_behavior()\n",
    "util = TextCNN_Util(dp=dp, model=D)\n",
    "util.fit(train_dir=train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
